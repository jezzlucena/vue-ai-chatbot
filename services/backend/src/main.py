from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ConfigDict
from pydantic.alias_generators import to_camel
from threading import Thread
from torch import cuda
from transformers import AutoModelForCausalLM, AutoTokenizer, AsyncTextIteratorStreamer
from typing import List, Literal, Optional
import json
import random

class CamelCaseModel(BaseModel):
    """Model that configures CamelCase alias generation,
    use this as base for other models as needed (e.g. when
    returning a json dict with FastAPI)"""
    model_config = ConfigDict(
        alias_generator=to_camel,
        populate_by_name=True,
        from_attributes=True
    )

class ChatMessage(CamelCaseModel):
    """Represents a message in the chat"""
    role: Literal['assistant', 'system', 'user']
    content: str
    color: Optional[str] = None

class ChatbotState(CamelCaseModel):
    """Encapsulates the state of the chatbot,
    for use when recovering the UI"""
    messages: List[ChatMessage]
    is_locked: bool

EOS_TOKEN = "<|im_end|>"
"""End of Sequence token, special token used
by Qwen to mark the end of a series of streamable
chunks in a message from the assistant"""

PARAGRAPH_TOKEN = "\n\n"
"""Sequence that represents a paragraph, used
to translate paragraphs into separate messages
during the message stream"""

INITIAL_COLOR = "rgb(59 130 246)"
"""Initial user color for the chat bubbles and UI,
inspired from Tailwind's bg-blue-500"""

MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
"""Name of the Hugging Face model that will be used"""

device = "cuda" if cuda.is_available() else "cpu"
"""Device that will process the compute, (cuda = GPU,
or cpu = CPU). Keep in mind that GPU processing tends
to be much more performant. Warning: CUDA drivers only
exist for Windows at this time"""

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype="auto",
    device_map=device
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

chat_messages: List[ChatMessage] = []
"""List of messages currently on the server. Messages
are not persisted, so this list is cleaned after every
reboot of the backend service"""

random_colors: List[str] = [INITIAL_COLOR]
"""List of random CSS colors, generated by :func:`random_dark_color`,
formatted as rgb(0-255, 0-255, 0-255)"""

color_index = 0
"""Current index of the color we will send next time a
user connects. Warning: this CAN be out of boundaries of
:var:`random_colors` by 1, in which case we generate
a new random color, return it, and increment this"""

is_locked = False
"""State variable that controls whether the chatbot's UI
is locked for the user. This is used whenever the assistant
is generating/streaming a message, to avoid messes (e.g.
simultaneous generation of tokens)"""

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8081", "https://chatbot.jezzlucena.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
"""CORS middleware, used to accept HTTP requests only from
allowed origins"""

def random_dark_color():
    """Function that generates random dark colors.
    
    1 - It starts by instantiating 3 random integers, one from 0-50, 
    a second one from 0-150, and a 3rd one from 0-255. These represent
    the color intensity that we will give to each of the color components
    (red, green, and blue).

    2 - It then appends these integers to a list, each associated with a
    random float from 0-1 called sort_factor, that will be used later.

    3 - We sort the list created on our last step by the random sort_factor.

    4 - Assign red, green, and blue variables with the respective color 
    component at indices 0, 1, and 2 (in this order).

    5 - Return formatted string"""
    low = random.randint(0, 50)
    mid = random.randint(0, 150)
    high = random.randint(0, 255)

    components = [
        { 'value': low, 'sort_factor': random.random() },
        { 'value': mid, 'sort_factor': random.random() },
        { 'value': high, 'sort_factor': random.random() }
    ]

    components.sort(key=lambda x: x['sort_factor'])

    red = components[0]['value']
    green = components[1]['value']
    blue = components[2]['value']

    return f"rgb({red}, {green}, {blue})"

class ConnectionManager:
    """Class that manages connection, disconnection, and broadcasting
    of messages to WebSocket clients"""
    def __init__(self):
        self.active_connections: list[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)

manager = ConnectionManager()
"""Singleton instance of ConnectionManager"""

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Async function that serves as an endpoint for WebSocket
    connections."""
    await manager.connect(websocket)
    """Start by accepting the user's connection and adding a
    reference to the ConnectionManager instance"""
    try:
        global is_locked
        global color_index
        """Use global directive to let the python interpreter
        know we are referring to variables instantiated in a
        global scope"""
        if (color_index >= len(random_colors)):
            random_colors.append(random_dark_color())
        await websocket.send_text(json.dumps({
            'type': "color",
            'content': random_colors[color_index]
        }))
        color_index += 1
        """Send a designated color for the user that just connected,
        generating a random color if needed."""
        while True:
            """WebSocket message loop, here's where most of the
            computation happens."""
            text = await websocket.receive_text()
            """Wait for the user to send a chat message or a command"""
            if (is_locked):
                """If the app is locked (e.g. because the assistant is
                currently broadcasting a message to the clients), ignore
                any other messages to avoid inconsistencies."""
                continue
            data = json.loads(text)
            """Parse the text received from the client into a dict"""

            match data['type']:
                case 'reset':
                    """Reset messages and generated random colors,
                    broadcast to all clients"""
                    del chat_messages [:]
                    del random_colors [:]
                    random_colors.append(INITIAL_COLOR)
                    color_index = 0
                    await manager.broadcast(json.dumps({ 'type': "reset" }))
                case 'userMessage':
                    """
                    1 - Append the new user message to the list.
                    2 - Broadcast the message with the appropriate color
                    to all clients.
                    3 - Lock the app.
                    """
                    chat_messages.append({ 'role': "user", 'content': data['content'], 'color': data['color'] })

                    await manager.broadcast(json.dumps({
                        'type': "userMessage",
                        'content': data['content'],
                        'color': data['color']
                    }))

                    is_locked = True

                    """
                    4 - Tokenize the input text and history
                    5 - Send the model inputs to the appropriate
                    device (cpu or gpu).
                    """
                    inputs = tokenizer.apply_chat_template(
                        chat_messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = tokenizer([inputs], return_tensors="pt").to(device)

                    """6 - Initiate the Async Streamer and Thread that
                    will be used during the generation of the response"""
                    decode_kwargs = dict(skip_special_tokens=True)
                    streamer = AsyncTextIteratorStreamer(tokenizer, skip_prompt=True, decode_kwargs=decode_kwargs)
                    generation_kwargs = dict(**model_inputs, streamer=streamer, max_new_tokens=512)
                    thread = Thread(target=model.generate, kwargs=generation_kwargs)
                    thread.start()

                    stream_started = False

                    """7 - Add interaction to conversation history preemptively"""
                    response = { "role": "assistant", "content": "" }
                    chat_messages.append(response)

                    async for chunk in streamer:
                        """
                        8.1 - Start streaming only when we get a token that is
                        different than an empty string.
                        8.2 - Sanitize the chunks, to remove end of sequence
                        tokens and transform paragraphs into new messages.
                        8.3 - Send sanitized chunk to all clients.
                        """
                        if (not stream_started and chunk != ""):
                            await manager.broadcast(json.dumps({
                                'type': "aiStart"
                            }))
                            stream_started = True
                        
                        word = chunk
                        if (chunk.endswith(EOS_TOKEN)):
                            word = word.split(EOS_TOKEN)[0]
                        elif (chunk.endswith(PARAGRAPH_TOKEN)):
                            word = word.split(PARAGRAPH_TOKEN)[0]
                        await manager.broadcast(json.dumps({
                            'type': "aiChunk",
                            'content': word
                        }))

                        """
                        8.4 - Append sanitized chunk to previously instantiated
                        message object.
                        8.5 - If this chunk ended in a paragraph, start a new
                        message. That way the chatbot communicates more like a
                        person texting.
                        """
                        response["content"] += word
                        
                        if (chunk.endswith(PARAGRAPH_TOKEN)):
                            response = { "role": "assistant", "content": "" }
                            chat_messages.append(response)
                            await manager.broadcast(json.dumps({
                                'type': "aiStart"
                            }))

                    """
                    8.6 - Send end of streaming signal to all clients.
                    8.7 - Unlock the app
                    """
                    await manager.broadcast(json.dumps({
                        'type': "aiEnd"
                    }))

                    is_locked = False
                case 'prompt':
                    """Create a system prompt, to inform to the assistant
                    how they should behave during this session. Broadcast
                    to all clients."""
                    chat_messages.append({ 'role': "system", 'content': data['content'] })
                    
                    await manager.broadcast(json.dumps({
                        'type': "prompt",
                        'content': data['content']
                    }))
                case 'typing':
                    """Simply broadcast to all clients that one of them
                    is typing, informing their respective color."""
                    await manager.broadcast(json.dumps({
                        'type': "typing",
                        'color': data['color']
                    }))
    except WebSocketDisconnect:
        """Disconnect client in case of exception"""
        manager.disconnect(websocket)

@app.get("/state")
def get_state() -> ChatbotState:
    """Returns the current state of the app (e.g. all messages
    currently in session, if the app is locked)."""
    return {
        'messages': chat_messages,
        'is_locked': is_locked
    }
